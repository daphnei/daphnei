<!DOCTYPE html>
<html lang="en">
<head>
        <title>Projects</title>
        <meta charset="utf-8" />
        <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />
   

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie.css"/>
                <script src="/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie6.css"/><![endif]-->

</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="">Daphne Ippolito </a></h1>
                <nav><ul>
                    <li ><a href="/category/about-me.html">About Me</a></li>
                    <li ><a href="/category/cv.html">CV</a></li>
                    <li class="active"><a href="/category/projects.html">Projects</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="projects.html" rel="bookmark"
           title="Permalink to Projects">Projects</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
	<!--
        <abbr class="published" title="2010-12-03T10:20:00-05:00">
                Fri 03 December 2010
        </abbr>

        <address class="vcard author">
                By <a class="url fn" href="/author/daphne-ippolito.html">Daphne Ippolito</a>
        </address>
<p>In <a href="/category/projects.html">Projects</a>. </p>
        -->
</footer><!-- /.post-info -->      <p>This page lists my ongoing and past research projects that I am excited by.</p>
<p>To see a full list of my papers, check out my <a href="https://scholar.google.com/citations?user=COEsqLYAAAAJ&amp;hl=en&amp;oi=ao">Google Scholar profile</a>.</p>
<h2>Language Model Memorization</h2>
<p><a href="https://arxiv.org/abs/2107.06499">paper</a> | <a href="https://github.com/google-research/deduplicate-text-datasets">code</a><br>
Large language models memorize their training data.
This is bad for many reasons.
We have so far shown that deduplicating the training set can signifiantly reduce memorization caused by an example being in the dataset many times.
We are continuing to study the properties of LM memorization, including membership inference attacks, the impact of decoding staretgy and prompt choice on whether memorized content surfaces, and training strategies to control the amount of memorization. </p>
<h2>Building Tools for Creative Writing</h2>
<p><a href="https://arxiv.org/abs/2107.07430">paper on Wordcraft</a> | <a href="https://arxiv.org/abs/2109.03910">paper on style transfer</a>
Can natural language generation systems be used to build tools for creative writing? Controllable rewriting, text elaboration and expansion,
and plot ideation are all tasks that NLG might be able to assist with. We are especially interested in investigating how creative writers interact
with and perceive such tools.</p>
<h2>The Real or Fake Text Game</h2>
<p><a href="http://roft.io/">play</a> | <a href="https://arxiv.org/abs/2010.03070">demo paper</a> | <a href="https://github.com/kirubarajan/roft/">code</a> <br>
Are you able to detect when a passage of text transitions from being being human-written to being machine-generated? Test out your skills on RoFT, the Real or Fake Text game.
The data from our game will be used to answer questions about how factors like genre, and decoding strategy, and annotator training impact the detectability of machine-generated text.</p>
<h2>The Diversity-Quality Tradeoff</h2>
<p><a href="https://arxiv.org/abs/1906.06362">paper on diverse decoding strategies</a> | <a href="https://arxiv.org/abs/1911.00650">paper on detection of machine-generated text</a><br>
It's possible to generate diverse text with a neural language mdodel or generate text humans perceive as high quality, but even with state-of-the-art language models, it's hard to do both.
We investigate this tradeoff and how it is impacted by choice of decoding strategy.
We also look at the detection problem: how good are humans (and trained classifiers) at detecting when text was generated with a language model?</p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>Links</h2>
                        <ul>
                            <li><a href="https://github.com/daphnei">Github</a></li>
                            <li><a href="https://scholar.google.com/citations?user=COEsqLYAAAAJ&hl=en">Google Scholar</a></li>
                            <li><a href="https://twitter.com/daphneipp">Twitter</a></li>
                        </ul>
                </div><!-- /.blogroll -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <div>
                        <p class="foot">Made with <a href="https://docs.getpelican.com/">Pelican</a> based on the <a href="https://github.com/getpelican/pelican-themes/tree/master/subtle">subtle</a> theme.</p>
                </div>
        </footer><!-- /#contentinfo -->

</body>
</html>